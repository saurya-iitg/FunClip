#!/usr/bin/env python3
# -*- encoding: utf-8 -*-
# Copyright FunASR (https://github.com/alibaba-damo-academy/FunClip). All Rights Reserved.
#  MIT License  (https://opensource.org/licenses/MIT)

from http import server
import os
import logging
import argparse
import gradio as gr
from funasr import AutoModel
from videoclipper import VideoClipper
from llm.openai_api import openai_call
from llm.qwen_api import call_local_model  #call_qwen_model,   # Corrected import
from llm.g4f_openai_api import g4f_openai_call
from utils.trans_utils import extract_timestamps
from introduction import top_md_1, top_md_3, top_md_4
import requests


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='argparse testing')
    parser.add_argument('--lang', '-l', type=str, default = "en", help="language")
    parser.add_argument('--share', '-s', action='store_true', help="if to establish gradio share link")
    parser.add_argument('--port', '-p', type=int, default=7860, help='port number')
    parser.add_argument('--listen', action='store_true', help="if to listen to all hosts")
    args = parser.parse_args()
    
    if args.lang == 'zh':
        funasr_model = AutoModel(
            model="iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
            vad_model="damo/speech_fsmn_vad_zh-cn-16k-common-pytorch",
            punc_model="damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch",
            spk_model="damo/speech_campplus_sv_zh-cn-16k-common",
        )
    else:
        funasr_model = AutoModel(
            model="iic/speech_paraformer_asr-en-16k-vocab4199-pytorch",
            vad_model="damo/speech_fsmn_vad_zh-cn-16k-common-pytorch",
            punc_model="damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch",
            spk_model=None  # Disable speaker model for English
        )
    audio_clipper = VideoClipper(funasr_model)
    audio_clipper.lang = args.lang
    
    server_name='127.0.0.1'
    if args.listen:
        server_name = '0.0.0.0'
        
        

    def audio_recog(audio_input, sd_switch, hotwords, output_dir, asr_model="funasr"):
        if asr_model.startswith("whisper"):
            # The format is "whisper-base", "whisper-small", etc.
            version = asr_model.split("-")[1]  
            recognized_text = whisper_recog(audio_input, version=version)
            res_srt = ""
            state = {"audio_input": audio_input}
            return recognized_text, res_srt, state
        else:
            # Use the original FunASR-based recognition
            return audio_clipper.recog(audio_input, sd_switch, None, hotwords, output_dir=output_dir)

    def video_recog(video_input, sd_switch, hotwords, output_dir):
        return audio_clipper.video_recog(video_input, sd_switch, hotwords, output_dir=output_dir)

    def video_clip(dest_text, video_spk_input, start_ost, end_ost, state, output_dir):
        return audio_clipper.video_clip(
            dest_text, start_ost, end_ost, state, dest_spk=video_spk_input, output_dir=output_dir
            )

    def mix_recog(video_input, audio_input, hotwords, output_dir, asr_model):
        output_dir = output_dir.strip()
        output_dir = None if not len(output_dir) else os.path.abspath(output_dir)
        if video_input is not None:
            res_text, res_srt, video_state = video_recog(video_input, 'No', hotwords, output_dir=output_dir)
            return res_text, res_srt, video_state, None
        if audio_input is not None:
            res_text, res_srt, audio_state = audio_recog(audio_input, 'No', hotwords, output_dir=output_dir, asr_model=asr_model)
            return res_text, res_srt, None, audio_state
    
    def mix_recog_speaker(video_input, audio_input, hotwords, output_dir):
        output_dir = output_dir.strip()
        if not len(output_dir):
            output_dir = None
        else:
            output_dir = os.path.abspath(output_dir)
        audio_state, video_state = None, None
        if video_input is not None:
            res_text, res_srt, video_state = video_recog(
                video_input, 'Yes', hotwords, output_dir=output_dir)
            return res_text, res_srt, video_state, None
        if audio_input is not None:
            res_text, res_srt, audio_state = audio_recog(
                audio_input, 'Yes', hotwords, output_dir=output_dir)
            return res_text, res_srt, None, audio_state
    
    def mix_clip(dest_text, video_spk_input, start_ost, end_ost, video_state, audio_state, output_dir):
        output_dir = output_dir.strip()
        if not len(output_dir):
            output_dir = None
        else:
            output_dir = os.path.abspath(output_dir)
        if video_state is not None:
            clip_video_file, message, clip_srt = audio_clipper.video_clip(
                dest_text, start_ost, end_ost, video_state, dest_spk=video_spk_input, output_dir=output_dir)
            return clip_video_file, None, message, clip_srt
        if audio_state is not None:
            (sr, res_audio), message, clip_srt = audio_clipper.clip(
                dest_text, start_ost, end_ost, audio_state, dest_spk=video_spk_input, output_dir=output_dir)
            return None, (sr, res_audio), message, clip_srt
    
    def video_clip_addsub(dest_text, video_spk_input, start_ost, end_ost, state, output_dir, font_size, font_color):
        output_dir = output_dir.strip()
        if not len(output_dir):
            output_dir = None
        else:
            output_dir = os.path.abspath(output_dir)
        return audio_clipper.video_clip(
            dest_text, start_ost, end_ost, state, 
            font_size=font_size, font_color=font_color, 
            add_sub=True, dest_spk=video_spk_input, output_dir=output_dir
            )
        
    def llm_inference(system_content, user_content, srt_text, model, apikey):
        """Handle different model types including LMStudio local models"""
        try:
            lmstudio_models = get_available_models()
            logging.info(f"Available models: {lmstudio_models}")
            
            combined_content = user_content + '\n' + srt_text
            
            if model.lower() in [m.lower() for m in lmstudio_models]:
                actual_model = next(m for m in lmstudio_models if m.lower() == model.lower())
                logging.info(f"Using local model: {actual_model}")
                return g4f_openai_call(actual_model, combined_content, system_content)
            elif model.startswith('gpt') or model.startswith('moonshot'):
                return openai_call(apikey, model, system_content, combined_content)
            else:
                error_msg = f"Unsupported model: {model}. Available models: {lmstudio_models}"
                logging.error(error_msg)
                return error_msg
        except Exception as e:
            error_msg = f"LLM inference error: {str(e)}"
            logging.error(error_msg)
            return error_msg
    
    def AI_clip(LLM_res, dest_text, video_spk_input, start_ost, end_ost, video_state, audio_state, output_dir):
        timestamp_list = extract_timestamps(LLM_res)
        output_dir = output_dir.strip()
        if not len(output_dir):
            output_dir = None
        else:
            output_dir = os.path.abspath(output_dir)
        if video_state is not None:
            clip_video_file, message, clip_srt = audio_clipper.video_clip(
                dest_text, start_ost, end_ost, video_state, 
                dest_spk=video_spk_input, output_dir=output_dir, timestamp_list=timestamp_list, add_sub=False)
            return clip_video_file, None, message, clip_srt
        if audio_state is not None:
            (sr, res_audio), message, clip_srt = audio_clipper.clip(
                dest_text, start_ost, end_ost, audio_state, 
                dest_spk=video_spk_input, output_dir=output_dir, timestamp_list=timestamp_list, add_sub=False)
            return None, (sr, res_audio), message, clip_srt
    
    def AI_clip_subti(LLM_res, dest_text, video_spk_input, start_ost, end_ost, video_state, audio_state, output_dir):
        timestamp_list = extract_timestamps(LLM_res)
        output_dir = output_dir.strip()
        if not len(output_dir):
            output_dir = None
        else:
            output_dir = os.path.abspath(output_dir)
        if video_state is not None:
            clip_video_file, message, clip_srt = audio_clipper.video_clip(
                dest_text, start_ost, end_ost, video_state, 
                dest_spk=video_spk_input, output_dir=output_dir, timestamp_list=timestamp_list, add_sub=True)
            return clip_video_file, None, message, clip_srt
        if audio_state is not None:
            (sr, res_audio), message, clip_srt = audio_clipper.clip(
                dest_text, start_ost, end_ost, audio_state, 
                dest_spk=video_spk_input, output_dir=output_dir, timestamp_list=timestamp_list, add_sub=True)
            return None, (sr, res_audio), message, clip_srt
    
    def get_available_models():
        """Get list of available models from LMStudio server"""
        try:
            response = requests.get("http://localhost:1234/v1/models")
            if response.status_code == 200:
                models = response.json()
                available_models = [model["id"].lower() for model in models.get("data", [])]
                logging.info(f"Available models: {available_models}")
                return available_models
            logging.warning(f"Failed to get models from LMStudio server: {response.status_code}")
            return []
        except Exception as e:
            logging.error(f"Failed to fetch models from LMStudio: {str(e)}")
            return []
    
    # gradio interface
    theme = gr.Theme.load("funclip/utils/theme.json")
    with gr.Blocks(theme=theme) as funclip_service:
        gr.Markdown(top_md_1)
        # gr.Markdown(top_md_2)
        gr.Markdown(top_md_3)
        gr.Markdown(top_md_4)
        video_state, audio_state = gr.State(), gr.State()
        with gr.Row():
            with gr.Column():
                with gr.Row():
                    video_input = gr.Video(label="è§†é¢‘è¾“å…¥ | Video Input")
                    audio_input = gr.Audio(label="éŸ³é¢‘è¾“å…¥ | Audio Input")
                with gr.Column():
                    gr.Examples(['https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%A4%9A%E8%AF%BB%E4%B9%A6%EF%BC%9F%E8%BF%99%E6%98%AF%E6%88%91%E5%90%AC%E8%BF%87%E6%9C%80%E5%A5%BD%E7%9A%84%E7%AD%94%E6%A1%88-%E7%89%87%E6%AE%B5.mp4', 
                                 'https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/2022%E4%BA%91%E6%A0%96%E5%A4%A7%E4%BC%9A_%E7%89%87%E6%AE%B52.mp4', 
                                 'https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/%E4%BD%BF%E7%94%A8chatgpt_%E7%89%87%E6%AE%B5.mp4'],
                                [video_input],
                                label='ç¤ºä¾‹è§†é¢‘ | Demo Video')
                    gr.Examples(['https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/%E8%AE%BF%E8%B0%88.mp4'],
                                [video_input],
                                label='å¤šè¯´è¯äººç¤ºä¾‹è§†é¢‘ | Multi-speaker Demo Video')
                    gr.Examples(['https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/%E9%B2%81%E8%82%83%E9%87%87%E8%AE%BF%E7%89%87%E6%AE%B51.wav'],
                                [audio_input],
                                label="ç¤ºä¾‹éŸ³é¢‘ | Demo Audio")
                    with gr.Column():
                        # with gr.Row():
                            # video_sd_switch = gr.Radio(["No", "Yes"], label="ğŸ‘¥åŒºåˆ†è¯´è¯äºº Get Speakers", value='No')
                        hotwords_input = gr.Textbox(label="ğŸš’ çƒ­è¯ | Hotwords(å¯ä»¥ä¸ºç©ºï¼Œå¤šä¸ªçƒ­è¯ä½¿ç”¨ç©ºæ ¼åˆ†éš”ï¼Œä»…æ”¯æŒä¸­æ–‡çƒ­è¯)")
                        output_dir = gr.Textbox(label="ğŸ“ æ–‡ä»¶è¾“å‡ºè·¯å¾„ | File Output Dir (å¯ä»¥ä¸ºç©ºï¼ŒLinux, macç³»ç»Ÿå¯ä»¥ç¨³å®šä½¿ç”¨)", value=" ")
                        asr_model = gr.Dropdown(
                            choices=["funasr", "whisper-base", "whisper-small", "whisper-medium", "whisper-large"],
                            value="funasr",
                            label="ASR Model"
                        )
                        with gr.Row():
                            recog_button = gr.Button("ğŸ‘‚ è¯†åˆ« | ASR", variant="primary")
                            recog_button2 = gr.Button("ğŸ‘‚ğŸ‘« è¯†åˆ«+åŒºåˆ†è¯´è¯äºº | ASR+SD")
                video_text_output = gr.Textbox(label="âœï¸ è¯†åˆ«ç»“æœ | Recognition Result")
                video_srt_output = gr.Textbox(label="ğŸ“– SRTå­—å¹•å†…å®¹ | RST Subtitles")
            with gr.Column():
                with gr.Tab("ğŸ§  LLMæ™ºèƒ½è£å‰ª | LLM Clipping"):
                    with gr.Column():
                        prompt_head = gr.Textbox(
                            label="Prompt System (Customize as needed, do not change the main instructions)",
                            value="""You are a video SRT subtitles analyzer and editor. 
Input the video's SRT subtitles, analyze them to identify the most interesting and continuous segments, 
and then cut out those segments. Output no more than four segments and merge any consecutive sentences 
and their timestamps into one segment. Ensure that the text matches the timestamps correctly. 
The output must strictly follow this format: 1. [start time-end time] text, note that the connector is "-"."""
                        )
                        prompt_head2 = gr.Textbox(label="Prompt Userï¼ˆä¸éœ€è¦ä¿®æ”¹ï¼Œä¼šè‡ªåŠ¨æ‹¼æ¥å·¦ä¸‹è§’çš„srtå­—å¹•ï¼‰", value=("This is the video srt subtitles to be croppedï¼š"))
                        with gr.Column():
                            with gr.Row():
                                llm_model = gr.Dropdown(
                                    choices=["gpt-3.5-turbo", "gpt-4"] + get_available_models(),
                                    value=get_available_models()[0] if get_available_models() else "gpt-3.5-turbo",
                                    label="LLM Model Name",
                                    allow_custom_value=False
                                )
                                apikey_input = gr.Textbox(label="APIKEY")
                            llm_button =  gr.Button("LLMæ¨ç† | LLM Inferenceï¼ˆé¦–å…ˆè¿›è¡Œè¯†åˆ«ï¼Œég4féœ€é…ç½®å¯¹åº”apikeyï¼‰", variant="primary")
                        llm_result = gr.Textbox(label="LLM Clipper Result")
                        with gr.Row():
                            llm_clip_button = gr.Button("ğŸ§  LLMæ™ºèƒ½è£å‰ª | AI Clip", variant="primary")
                            llm_clip_subti_button = gr.Button("ğŸ§  LLMæ™ºèƒ½è£å‰ª+å­—å¹• | AI Clip+Subtitles")
                with gr.Tab("âœ‚ï¸ æ ¹æ®æ–‡æœ¬/è¯´è¯äººè£å‰ª | Text/Speaker Clipping"):
                    video_text_input = gr.Textbox(label="âœï¸ å¾…è£å‰ªæ–‡æœ¬ | Text to Clip (å¤šæ®µæ–‡æœ¬ä½¿ç”¨'#'è¿æ¥)")
                    video_spk_input = gr.Textbox(label="âœï¸ å¾…è£å‰ªè¯´è¯äºº | Speaker to Clip (å¤šä¸ªè¯´è¯äººä½¿ç”¨'#'è¿æ¥)")
                    with gr.Row():
                        clip_button = gr.Button("âœ‚ï¸ è£å‰ª | Clip", variant="primary")
                        clip_subti_button = gr.Button("âœ‚ï¸ è£å‰ª+å­—å¹• | Clip+Subtitles")
                    with gr.Row():
                        video_start_ost = gr.Slider(minimum=-500, maximum=1000, value=0, step=50, label="âª å¼€å§‹ä½ç½®åç§» | Start Offset (ms)")
                        video_end_ost = gr.Slider(minimum=-500, maximum=1000, value=100, step=50, label="â© ç»“æŸä½ç½®åç§» | End Offset (ms)")
                with gr.Row():
                    font_size = gr.Slider(minimum=10, maximum=100, value=32, step=2, label="ğŸ”  å­—å¹•å­—ä½“å¤§å° | Subtitle Font Size")
                    font_color = gr.Radio(["black", "white", "green", "red"], label="ğŸŒˆ å­—å¹•é¢œè‰² | Subtitle Color", value='white')
                    # font = gr.Radio(["é»‘ä½“", "Alibaba Sans"], label="å­—ä½“ Font")
                video_output = gr.Video(label="è£å‰ªç»“æœ | Video Clipped")
                audio_output = gr.Audio(label="è£å‰ªç»“æœ | Audio Clipped")
                clip_message = gr.Textbox(label="âš ï¸ è£å‰ªä¿¡æ¯ | Clipping Log")
                srt_clipped = gr.Textbox(label="ğŸ“– è£å‰ªéƒ¨åˆ†SRTå­—å¹•å†…å®¹ | Clipped RST Subtitles")            
                
        recog_button.click(mix_recog, 
                            inputs=[video_input, 
                                    audio_input, 
                                    hotwords_input, 
                                    output_dir,
                                    asr_model], 
                            outputs=[video_text_output, video_srt_output, video_state, audio_state])
        recog_button2.click(mix_recog_speaker, 
                            inputs=[video_input, 
                                    audio_input, 
                                    hotwords_input, 
                                    output_dir,
                                    ], 
                            outputs=[video_text_output, video_srt_output, video_state, audio_state])
        clip_button.click(mix_clip, 
                           inputs=[video_text_input, 
                                   video_spk_input, 
                                   video_start_ost, 
                                   video_end_ost, 
                                   video_state, 
                                   audio_state, 
                                   output_dir
                                   ],
                           outputs=[video_output, audio_output, clip_message, srt_clipped])
        clip_subti_button.click(video_clip_addsub, 
                           inputs=[video_text_input, 
                                   video_spk_input, 
                                   video_start_ost, 
                                   video_end_ost, 
                                   video_state, 
                                   output_dir, 
                                   font_size, 
                                   font_color,
                                   ], 
                           outputs=[video_output, clip_message, srt_clipped])
        llm_button.click(llm_inference,
                         inputs=[prompt_head, prompt_head2, video_srt_output, llm_model, apikey_input],
                         outputs=[llm_result])
        llm_clip_button.click(AI_clip, 
                           inputs=[llm_result,
                                   video_text_input, 
                                   video_spk_input, 
                                   video_start_ost, 
                                   video_end_ost, 
                                   video_state, 
                                   audio_state, 
                                   output_dir,
                                   ],
                           outputs=[video_output, audio_output, clip_message, srt_clipped])
        llm_clip_subti_button.click(AI_clip_subti, 
                           inputs=[llm_result,
                                   video_text_input, 
                                   video_spk_input, 
                                   video_start_ost, 
                                   video_end_ost, 
                                   video_state, 
                                   audio_state, 
                                   output_dir,
                                   ],
                           outputs=[video_output, audio_output, clip_message, srt_clipped])
    
    # start gradio service in local or share
    if args.listen:
        funclip_service.launch(share=args.share, server_port=args.port, server_name=server_name, inbrowser=False)
    else:
        funclip_service.launch(share=args.share, server_port=args.port, server_name=server_name)
